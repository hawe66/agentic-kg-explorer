# Phase 5: Prompt Optimizer â€” Design Document

> Phase 5ëŠ” Critic Agentì˜ í‰ê°€ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ ìµœì í™”í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•œë‹¤.
> Human-in-the-Loop ì´ì¤‘ ê²Œì´íŠ¸ë¡œ ì•ˆì „í•˜ê²Œ í”„ë¡¬í”„íŠ¸ ì§„í™”ë¥¼ ê´€ë¦¬í•œë‹¤.

---

## 1. ëª©í‘œ ìš”ì•½

1. **Failure Analyzer** â€” ë°˜ë³µ ì‹¤íŒ¨ íŒ¨í„´ íƒì§€ ë° FailurePattern ìƒì„±
2. **Variant Generator** â€” ì‹¤íŒ¨ ê°€ì„¤ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ë³€í˜• ìƒì„± (3ê°œ)
3. **Test Runner** â€” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ë³€í˜• í‰ê°€, ìµœê³  ì„±ëŠ¥ ì„ íƒ
4. **Prompt Registry** â€” í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬, í™œì„±í™”/ë¡¤ë°±
5. **Review UI** â€” Human-in-the-Loop ìŠ¹ì¸ ê²Œì´íŠ¸

---

## 2. ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EVALUATION ACCUMULATION                      â”‚
â”‚  Phase 4 Critic â†’ Evaluation nodes in Neo4j (NíšŒ ì¶•ì )           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                         trigger (N >= threshold)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FAILURE ANALYZER                             â”‚
â”‚  - Query low-scoring evaluations by agent                        â”‚
â”‚  - Cluster by criterion (ec:source-citation, ec:reasoning, etc.) â”‚
â”‚  - Generate FailurePattern with root_cause_hypotheses            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GATE 1: HYPOTHESIS REVIEW                    â”‚
â”‚  User reviews/edits root_cause_hypotheses before proceeding      â”‚
â”‚  Options: Approve / Edit / Reject / Add more hypotheses          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                         (approved)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VARIANT GENERATOR                            â”‚
â”‚  - Load current prompt for affected agent                        â”‚
â”‚  - LLM generates 3 prompt variants addressing hypotheses         â”‚
â”‚  - Each variant has: diff, rationale, expected_improvement       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TEST RUNNER                                  â”‚
â”‚  - Run test queries (from config/test_queries.yaml)              â”‚
â”‚  - Evaluate each variant with Critic                             â”‚
â”‚  - Compare composite scores vs baseline                          â”‚
â”‚  - Rank variants by performance_delta                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GATE 2: PROMPT APPROVAL                      â”‚
â”‚  User reviews best variant:                                      â”‚
â”‚  - Side-by-side diff with current prompt                         â”‚
â”‚  - Test results summary                                          â”‚
â”‚  - Options: Approve / Edit / Reject / Try another variant        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                         (approved)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PROMPT REGISTRY                              â”‚
â”‚  - Create PromptVersion node in Neo4j                            â”‚
â”‚  - Link: (PromptVersion)-[:ADDRESSES]->(FailurePattern)          â”‚
â”‚  - Set is_active=true, deactivate previous version               â”‚
â”‚  - Update prompt file on disk                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Graph Schema (ì´ë¯¸ Phase 4ì—ì„œ ì¶”ê°€ë¨)

```cypher
// FailurePattern: ë°˜ë³µ ì‹¤íŒ¨ íŒ¨í„´
CREATE CONSTRAINT fp_id IF NOT EXISTS
FOR (fp:FailurePattern) REQUIRE fp.id IS UNIQUE;

CREATE INDEX fp_type IF NOT EXISTS
FOR (fp:FailurePattern) ON (fp.pattern_type);

// PromptVersion: í”„ë¡¬í”„íŠ¸ ë²„ì „
CREATE CONSTRAINT pv_id IF NOT EXISTS
FOR (pv:PromptVersion) REQUIRE pv.id IS UNIQUE;

CREATE INDEX pv_agent IF NOT EXISTS
FOR (pv:PromptVersion) ON (pv.agent_name);

CREATE INDEX pv_active IF NOT EXISTS
FOR (pv:PromptVersion) ON (pv.is_active);

// Relationships
// (FailurePattern)-[:IDENTIFIED_FROM]->(Evaluation)
// (PromptVersion)-[:ADDRESSES]->(FailurePattern)
// (PromptVersion)-[:PARENT_OF]->(PromptVersion)
```

### Node Properties

```yaml
FailurePattern:
  id: string              # "fp:synthesizer:source-citation:2026-02"
  pattern_type: string    # "output_quality" | "reasoning" | "retrieval" | "classification"
  agent_name: string      # "synthesizer"
  criterion_id: string    # "ec:source-citation"
  description: string     # "Synthesizer consistently fails to cite KG sources"
  frequency: int          # Number of low-score evaluations
  sample_queries: [string] # Example queries that triggered this
  avg_score: float        # Average score for this criterion
  root_cause_hypotheses: [string]  # LLM-generated hypotheses
  suggested_fixes: [string]        # Potential prompt changes
  status: string          # "detected" | "reviewing" | "addressing" | "resolved"
  created_at: datetime
  resolved_at: datetime

PromptVersion:
  id: string              # "pv:synthesizer@1.2.0"
  agent_name: string      # "synthesizer"
  version: string         # "1.2.0" (semver)
  prompt_hash: string     # SHA256 of prompt content
  prompt_path: string     # "prompts/synthesizer/v1.2.0.txt"
  is_active: boolean      # Only one active per agent
  user_approved: boolean  # Human approved this version
  parent_version: string  # "pv:synthesizer@1.1.0"
  performance_delta: float # +0.05 improvement from parent
  test_results: string    # JSON summary of test run
  rationale: string       # Why this change was made
  created_at: datetime
  approved_at: datetime
  approved_by: string     # User identifier
```

---

## 4. íŒŒì¼ êµ¬ì¡°

```
src/
â”œâ”€â”€ optimizer/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ analyzer.py       # FailureAnalyzer class
â”‚   â”œâ”€â”€ generator.py      # VariantGenerator class
â”‚   â”œâ”€â”€ runner.py         # TestRunner class
â”‚   â”œâ”€â”€ registry.py       # PromptRegistry class
â”‚   â””â”€â”€ models.py         # Dataclasses (FailurePattern, PromptVariant, etc.)
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py            # Add optimization review panels
config/
â”œâ”€â”€ test_queries.yaml     # Test queries per intent type
â””â”€â”€ prompts/              # Versioned prompt storage
    â”œâ”€â”€ synthesizer/
    â”‚   â”œâ”€â”€ current.txt   # Symlink to active version
    â”‚   â”œâ”€â”€ v1.0.0.txt
    â”‚   â””â”€â”€ v1.1.0.txt
    â”œâ”€â”€ intent_classifier/
    â””â”€â”€ search_planner/
scripts/
â”œâ”€â”€ analyze_failures.py   # CLI to trigger failure analysis
â”œâ”€â”€ generate_variants.py  # CLI to generate prompt variants
â””â”€â”€ run_optimization.py   # Full optimization pipeline
```

---

## 5. ì»´í¬ë„ŒíŠ¸ ìƒì„¸

### 5.1 Failure Analyzer (`src/optimizer/analyzer.py`)

```python
class FailureAnalyzer:
    """Detect recurring failure patterns from evaluations."""

    def __init__(self, threshold: float = 0.6, min_samples: int = 5):
        self.threshold = threshold  # Score below this = failure
        self.min_samples = min_samples  # Min failures to create pattern

    def analyze(self, agent_name: str = None) -> list[FailurePattern]:
        """Query evaluations, cluster failures, generate patterns."""
        # 1. Query low-scoring evaluations from Neo4j
        # 2. Group by (agent_name, criterion_id)
        # 3. For groups with count >= min_samples:
        #    - Generate FailurePattern
        #    - Use LLM to hypothesize root causes
        # 4. Save FailurePattern to Neo4j
        pass

    def _generate_hypotheses(
        self,
        agent_name: str,
        criterion: str,
        sample_queries: list[str],
        sample_responses: list[str],
    ) -> list[str]:
        """Use LLM to generate root cause hypotheses."""
        prompt = f"""
        The {agent_name} agent consistently scores low on "{criterion}".

        Sample failing queries and responses:
        {self._format_samples(sample_queries, sample_responses)}

        Generate 2-3 hypotheses for why this might be happening.
        Focus on prompt-level issues that could be fixed.

        Output as JSON list: ["hypothesis 1", "hypothesis 2", ...]
        """
        # LLM call
        pass
```

### 5.2 Variant Generator (`src/optimizer/generator.py`)

```python
class VariantGenerator:
    """Generate prompt variants to address failure patterns."""

    def generate_variants(
        self,
        failure_pattern: FailurePattern,
        num_variants: int = 3,
    ) -> list[PromptVariant]:
        """Generate prompt variants addressing the failure."""
        # 1. Load current prompt for agent
        current_prompt = self._load_current_prompt(failure_pattern.agent_name)

        # 2. Generate variants with LLM
        prompt = f"""
        Current prompt for {failure_pattern.agent_name}:
        ---
        {current_prompt}
        ---

        This prompt has a recurring issue: {failure_pattern.description}

        Root cause hypotheses:
        {failure_pattern.root_cause_hypotheses}

        Generate {num_variants} improved versions of this prompt.
        Each version should:
        1. Address at least one hypothesis
        2. Be a complete replacement prompt
        3. Include a brief rationale

        Output as JSON:
        [
          {{
            "prompt": "full new prompt text...",
            "rationale": "why this change helps",
            "addresses_hypotheses": [0, 1]
          }},
          ...
        ]
        """
        # LLM call, parse variants
        pass
```

### 5.3 Test Runner (`src/optimizer/runner.py`)

```python
class TestRunner:
    """Run test queries and evaluate prompt variants."""

    def __init__(self):
        self.evaluator = get_evaluator()

    def run_tests(
        self,
        agent_name: str,
        variants: list[PromptVariant],
        test_queries: list[str] = None,
    ) -> list[TestResult]:
        """Run tests for each variant, return ranked results."""
        if test_queries is None:
            test_queries = self._load_test_queries(agent_name)

        results = []

        # Test baseline (current prompt)
        baseline_scores = self._test_variant(agent_name, None, test_queries)

        # Test each variant
        for variant in variants:
            variant_scores = self._test_variant(agent_name, variant, test_queries)
            delta = self._calculate_delta(baseline_scores, variant_scores)

            results.append(TestResult(
                variant=variant,
                scores=variant_scores,
                baseline_scores=baseline_scores,
                performance_delta=delta,
            ))

        # Rank by performance delta
        results.sort(key=lambda r: r.performance_delta, reverse=True)
        return results

    def _test_variant(
        self,
        agent_name: str,
        variant: PromptVariant | None,
        test_queries: list[str],
    ) -> dict[str, float]:
        """Run test queries with a specific prompt variant."""
        # Temporarily swap prompt if variant provided
        # Run queries through pipeline
        # Collect evaluation scores
        # Return average scores per criterion
        pass
```

### 5.4 Prompt Registry (`src/optimizer/registry.py`)

```python
class PromptRegistry:
    """Manage prompt versions and activation."""

    def __init__(self, prompts_dir: Path = None):
        self.prompts_dir = prompts_dir or Path("config/prompts")

    def get_current_version(self, agent_name: str) -> PromptVersion:
        """Get currently active prompt version."""
        pass

    def get_version_history(self, agent_name: str) -> list[PromptVersion]:
        """Get all versions for an agent."""
        pass

    def create_version(
        self,
        agent_name: str,
        content: str,
        parent_version: str,
        failure_pattern_id: str,
        rationale: str,
        test_results: dict,
    ) -> PromptVersion:
        """Create new prompt version (not yet active)."""
        # 1. Calculate next version number
        # 2. Write prompt to file
        # 3. Create PromptVersion node in Neo4j
        # 4. Link to FailurePattern
        pass

    def activate_version(self, version_id: str, approved_by: str) -> bool:
        """Activate a prompt version (deactivate previous)."""
        # 1. Set is_active=false on current active
        # 2. Set is_active=true, user_approved=true on new version
        # 3. Update symlink: current.txt -> vX.Y.Z.txt
        pass

    def rollback(self, agent_name: str, to_version: str = None) -> bool:
        """Rollback to previous version."""
        # If to_version not specified, rollback to parent
        pass
```

---

## 6. Test Queries Configuration

```yaml
# config/test_queries.yaml

synthesizer:
  - query: "What is ReAct?"
    expected_intent: lookup
    expected_entities: ["m:react"]
    min_confidence: 0.7

  - query: "What methods address Planning?"
    expected_intent: path
    min_sources: 2

  - query: "Compare LangChain and CrewAI"
    expected_intent: comparison
    expected_entities: ["impl:langchain", "impl:crewai"]

intent_classifier:
  - query: "What is CoT?"
    expected_intent: lookup
    expected_entities: ["m:cot"]

  - query: "How many methods are there?"
    expected_intent: aggregation

  - query: "What's the weather?"
    expected_intent: out_of_scope

search_planner:
  - query: "Which frameworks implement ReAct?"
    expected_template: "path_method_to_impl"
    expected_retrieval: "hybrid"

graph_retriever:
  - query: "What is ReAct?"
    min_results: 1
    no_error: true
```

---

## 7. UI Components

### 7.1 Failure Patterns Panel (Sidebar)

```python
# In src/ui/app.py

with st.sidebar:
    st.header("ğŸ” Failure Patterns")

    # Show detected patterns
    patterns = get_failure_patterns(status="detected")

    for fp in patterns:
        with st.expander(f"{fp.agent_name}: {fp.criterion_id}"):
            st.write(f"**Frequency:** {fp.frequency}")
            st.write(f"**Avg Score:** {fp.avg_score:.2f}")
            st.write("**Hypotheses:**")
            for h in fp.root_cause_hypotheses:
                st.write(f"- {h}")

            if st.button("Start Optimization", key=f"opt_{fp.id}"):
                st.session_state.optimizing_pattern = fp
```

### 7.2 Gate 1: Hypothesis Review

```python
if st.session_state.get("optimizing_pattern"):
    fp = st.session_state.optimizing_pattern

    st.subheader("Gate 1: Review Hypotheses")
    st.write(f"Pattern: {fp.description}")

    # Editable hypotheses
    edited_hypotheses = []
    for i, h in enumerate(fp.root_cause_hypotheses):
        edited = st.text_input(f"Hypothesis {i+1}", value=h, key=f"hyp_{i}")
        edited_hypotheses.append(edited)

    # Add new hypothesis
    new_hyp = st.text_input("Add hypothesis", key="new_hyp")
    if new_hyp:
        edited_hypotheses.append(new_hyp)

    col1, col2 = st.columns(2)
    with col1:
        if st.button("Approve & Generate Variants"):
            fp.root_cause_hypotheses = edited_hypotheses
            variants = generator.generate_variants(fp)
            st.session_state.pending_variants = variants
    with col2:
        if st.button("Reject Pattern"):
            mark_pattern_rejected(fp.id)
            st.session_state.optimizing_pattern = None
```

### 7.3 Gate 2: Prompt Approval

```python
if st.session_state.get("test_results"):
    results = st.session_state.test_results
    best = results[0]  # Already sorted by performance

    st.subheader("Gate 2: Approve Prompt Change")

    # Show diff
    st.write("**Prompt Diff:**")
    current = registry.get_current_version(best.variant.agent_name)
    diff = generate_diff(current.content, best.variant.prompt)
    st.code(diff, language="diff")

    # Show test results
    st.write(f"**Performance Delta:** +{best.performance_delta:.2%}")
    st.write("**Test Scores:**")
    st.json(best.scores)

    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("âœ… Approve & Activate"):
            registry.activate_version(best.version_id, user="streamlit")
            st.success("New prompt activated!")
    with col2:
        if st.button("âœï¸ Edit"):
            st.session_state.editing_prompt = best.variant
    with col3:
        if st.button("âŒ Reject"):
            st.session_state.test_results = None
```

---

## 8. êµ¬í˜„ ìˆœì„œ

### Step 1: Models & Registry (Day 1)
- [ ] `src/optimizer/models.py` â€” Dataclasses
- [ ] `src/optimizer/registry.py` â€” PromptRegistry
- [ ] `config/prompts/` â€” Initial prompt files extracted from code

### Step 2: Failure Analyzer (Day 2)
- [ ] `src/optimizer/analyzer.py` â€” FailureAnalyzer
- [ ] `scripts/analyze_failures.py` â€” CLI tool

### Step 3: Variant Generator (Day 3)
- [ ] `src/optimizer/generator.py` â€” VariantGenerator
- [ ] `config/test_queries.yaml` â€” Test query definitions

### Step 4: Test Runner (Day 4)
- [ ] `src/optimizer/runner.py` â€” TestRunner
- [ ] Integration with Critic evaluator

### Step 5: UI Integration (Day 5)
- [ ] Gate 1: Hypothesis review panel
- [ ] Gate 2: Prompt approval panel
- [ ] Failure patterns sidebar

### Step 6: Testing & Polish (Day 6)
- [ ] End-to-end test: failure â†’ hypothesis â†’ variant â†’ test â†’ approve
- [ ] Rollback functionality
- [ ] Update CHANGELOG.md

---

## 9. API Endpoints (Optional)

```python
# In src/api/routes.py

@router.get("/failure-patterns")
def get_failure_patterns(status: str = None, agent: str = None):
    """List failure patterns."""
    pass

@router.post("/failure-patterns/{pattern_id}/approve-hypotheses")
def approve_hypotheses(pattern_id: str, hypotheses: list[str]):
    """Gate 1: Approve hypotheses and trigger variant generation."""
    pass

@router.get("/prompt-versions")
def get_prompt_versions(agent: str = None, active_only: bool = False):
    """List prompt versions."""
    pass

@router.post("/prompt-versions/{version_id}/activate")
def activate_prompt_version(version_id: str):
    """Gate 2: Activate approved prompt version."""
    pass

@router.post("/prompt-versions/{version_id}/rollback")
def rollback_prompt(agent: str, to_version: str = None):
    """Rollback to previous prompt version."""
    pass
```

---

## 10. ë¦¬ìŠ¤í¬ & ì™„í™”

| Risk | Mitigation |
|------|------------|
| LLM generates poor variants | Human review gate before activation |
| Test queries not representative | Maintain diverse test set, allow user additions |
| Regression after activation | Easy rollback, track performance_delta |
| Over-optimization (overfitting) | Cross-validation with held-out queries |
| Prompt drift over versions | Version history, parent tracking, diff view |

---

## 11. ì´ë¡ ì  ê¸°ë°˜

| Paper | Key Idea | Our Application |
|-------|----------|-----------------|
| **APO** [Pryzant 2023] | Text gradients from failures | root_cause_hypotheses as gradients |
| **Self-Refine** [Madaan 2023] | Generate-Feedback-Refine | Critic feedback â†’ Generator â†’ Test |
| **Reflexion** [Shinn 2023] | Verbal reinforcement | Failure patterns as verbal feedback |
| **PromptWizard** [Agarwal 2024] | Instruction-Example co-optimization | Could extend to example selection |

---

## 12. Success Metrics

- **Pattern Resolution Rate**: % of FailurePatterns that reach "resolved" status
- **Prompt Improvement**: Average performance_delta of approved versions
- **Approval Rate**: % of generated variants that get approved (measures generation quality)
- **Rollback Rate**: % of activations that need rollback (should be low)
- **Time to Resolution**: Days from pattern detection to resolution

---

## 13. Scope: Prompts vs Cypher Templates

### Current Scope (Phase 5)

Phase 5 optimizes **LLM instruction prompts only**, not Cypher query templates.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IN SCOPE: LLM Prompts              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ intent_classifier prompt         â”‚
â”‚  â€¢ search_planner prompt            â”‚
â”‚  â€¢ synthesizer prompt               â”‚
â”‚  â€¢ graph_retriever prompt           â”‚
â”‚                                     â”‚
â”‚  Location: src/agents/nodes/*.py    â”‚
â”‚  â†’ Extract to: config/prompts/      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUT OF SCOPE: Cypher Templates     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ lookup_method                    â”‚
â”‚  â€¢ path_principle_methods           â”‚
â”‚  â€¢ comparison_impl                  â”‚
â”‚  â€¢ aggregation_* templates          â”‚
â”‚                                     â”‚
â”‚  Location: config/cypher_templates.yaml â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Rationale for Separation

| Aspect | LLM Prompts | Cypher Templates |
|--------|-------------|------------------|
| **Nature** | Natural language instructions | Structured query patterns |
| **Optimization** | Style, clarity, examples | Correctness, efficiency |
| **LLM can improve?** | Yes (text generation) | Limited (needs schema knowledge) |
| **Failure mode** | Misunderstanding, verbosity | Wrong results, syntax errors |
| **Testing** | Subjective quality scores | Deterministic result validation |

### When Retrieval Fails

If `graph_retriever` scores low, the root cause is usually:

1. **Wrong template selected** â†’ Fix `search_planner` prompt (IN SCOPE)
2. **Template itself is wrong** â†’ Manual fix to YAML (OUT OF SCOPE)
3. **Entity not in KG** â†’ Data issue, not optimization target

---

## 14. Future Extension: Cypher Template Optimization

> **Status**: Not planned for Phase 5. Consider for Phase 6+.

### Potential Approach

If we extend to Cypher optimization, it would be a **separate track**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CYPHER TEMPLATE OPTIMIZER                       â”‚
â”‚                  (Future Phase 6+)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Template        â”‚  â”‚ Template        â”‚  â”‚ Template        â”‚
â”‚ Selection       â”‚  â”‚ Correctness     â”‚  â”‚ Performance     â”‚
â”‚ Optimizer       â”‚  â”‚ Validator       â”‚  â”‚ Tuner           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                    â”‚                    â”‚
â”‚ - Which template   â”‚ - Does query      â”‚ - Index usage
â”‚   for which intent â”‚   return expected â”‚ - LIMIT values
â”‚ - Entity type      â”‚   schema?         â”‚ - Query complexity
â”‚   detection rules  â”‚ - Syntax valid?   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Cypher-Specific Challenges

1. **Schema Awareness**: LLM needs to know node labels, relationship types, property names
2. **Syntax Validation**: Must verify Cypher is executable before testing
3. **Result Validation**: Need ground-truth expected results, not just scores
4. **Performance Metrics**: Query execution time, index usage (requires EXPLAIN/PROFILE)

### Potential CypherTemplateVersion Schema

```yaml
CypherTemplateVersion:
  id: string              # "ct:lookup_method@1.2.0"
  template_name: string   # "lookup_method"
  version: string
  cypher: string          # The actual Cypher query
  parameters: [string]    # ["entity"]
  expected_labels: [string]  # ["Method"] - for validation
  is_active: boolean
  avg_execution_time_ms: float
  created_at: datetime
```

### When to Consider This

- After Phase 5 is stable and showing value
- If retrieval failures remain high despite prompt optimization
- If new entity types/relationships are frequently added
- If query performance becomes a bottleneck
