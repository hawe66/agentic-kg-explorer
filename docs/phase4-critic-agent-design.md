# Phase 4: Critic Agent + P2 Document Pipeline ‚Äî Design Document

> Phase 4Îäî Critic Agent ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌïòÍ≥†, P2Ïùò Î¨∏ÏÑú ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ìï®Íªò Íµ¨ÌòÑÌïúÎã§.
> Critic AgentÍ∞Ä Î¨∏ÏÑú ÌíàÏßàÍ≥º KG Ïó∞Í≤∞ÏùÑ ÌèâÍ∞ÄÌï† Ïàò ÏûàÏúºÎØÄÎ°ú ÏûêÏó∞Ïä§Îü¨Ïö¥ Í≤∞Ìï©Ïù¥Îã§.

---

## 1. Î™©Ìëú ÏöîÏïΩ

### Phase 4: Critic Agent
1. **EvaluationCriteria** ‚Äî 11 PrinciplesÏóêÏÑú ÎèÑÏ∂úÌïú ÌèâÍ∞Ä Í∏∞Ï§Ä
2. **Evaluation** ‚Äî ÏóêÏù¥Ï†ÑÌä∏ Ï∂úÎ†• ÌèâÍ∞Ä Î∞è Í≤∞Í≥º Ï†ÄÏû•
3. **FailurePattern** ‚Äî Î∞òÎ≥µ Ïã§Ìå® Ìå®ÌÑ¥ ÌÉêÏßÄ (Phase 5 Ï§ÄÎπÑ)
4. **Guideline Versioning** ‚Äî ÌîÑÎ°¨ÌîÑÌä∏ Î≤ÑÏ†Ñ Í¥ÄÎ¶¨

### P2: Document Pipeline
1. **Î≤îÏö© Î¨∏ÏÑú ÌÅ¨Î°§Îü¨** ‚Äî URL/PDFÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
2. **Local docs ÏóÖÎ°úÎìú UI** ‚Äî StreamlitÏóêÏÑú ÌååÏùº ÏóÖÎ°úÎìú
3. **Document ‚Üí KG ÏûêÎèô Ïó∞Í≤∞** ‚Äî LLMÏúºÎ°ú Î¨∏ÏÑú-Method/Implementation Í¥ÄÍ≥Ñ Ï∂îÏ∂ú

---

## 2. ÌÜµÌï© ÏïÑÌÇ§ÌÖçÏ≤ò

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     DOCUMENT PIPELINE (P2)                      ‚îÇ
‚îÇ  URL/PDF ‚Üí Crawler ‚Üí Chunking ‚Üí Embedding ‚Üí ChromaDB            ‚îÇ
‚îÇ                         ‚Üì                                        ‚îÇ
‚îÇ                   LLM Extraction                                 ‚îÇ
‚îÇ                         ‚Üì                                        ‚îÇ
‚îÇ              Document ‚Üí KG Linkage                               ‚îÇ
‚îÇ         (PROPOSES Method, DESCRIBES Implementation)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     RUNTIME PIPELINE                            ‚îÇ
‚îÇ  User Query ‚Üí Intent ‚Üí Search ‚Üí Retrieve ‚Üí Web ‚Üí Synthesize     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CRITIC EVALUATION                           ‚îÇ
‚îÇ  - Answer quality scoring (4-dim ‚Üí N-dim)                       ‚îÇ
‚îÇ  - Data sufficiency check                                       ‚îÇ
‚îÇ  - Save Evaluation node to Neo4j                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                         Ï∂ïÏ†Å (NÌöå)
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PATTERN ANALYSIS (Phase 5)                   ‚îÇ
‚îÇ  - Detect FailurePatterns                                       ‚îÇ
‚îÇ  - Generate PromptVersion candidates                            ‚îÇ
‚îÇ  - Human approval gate                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 3. Graph Schema ÌôïÏû•

### 3.1 Neo4j Schema (`neo4j/schema.cypher` Ï∂îÍ∞Ä)

```cypher
// =============================================
// Phase 4: Critic Agent Schema
// =============================================

// EvaluationCriteria: ÌèâÍ∞Ä Í∏∞Ï§Ä (PrincipleÏóêÏÑú ÎèÑÏ∂ú)
CREATE CONSTRAINT ec_id IF NOT EXISTS FOR (ec:EvaluationCriteria) REQUIRE ec.id IS UNIQUE;
CREATE INDEX ec_principle IF NOT EXISTS FOR (ec:EvaluationCriteria) ON (ec.principle_id);
CREATE INDEX ec_agent IF NOT EXISTS FOR (ec:EvaluationCriteria) ON (ec.agent_target);

// Evaluation: Í∞úÎ≥Ñ ÌèâÍ∞Ä Í≤∞Í≥º
CREATE CONSTRAINT eval_id IF NOT EXISTS FOR (e:Evaluation) REQUIRE e.id IS UNIQUE;
CREATE INDEX eval_agent IF NOT EXISTS FOR (e:Evaluation) ON (e.agent_name);
CREATE INDEX eval_created IF NOT EXISTS FOR (e:Evaluation) ON (e.created_at);

// FailurePattern: Î∞òÎ≥µ Ïã§Ìå® Ìå®ÌÑ¥ (Phase 5)
CREATE CONSTRAINT fp_id IF NOT EXISTS FOR (fp:FailurePattern) REQUIRE fp.id IS UNIQUE;

// PromptVersion: ÌîÑÎ°¨ÌîÑÌä∏ Î≤ÑÏ†Ñ (Phase 5)
CREATE CONSTRAINT pv_id IF NOT EXISTS FOR (pv:PromptVersion) REQUIRE pv.id IS UNIQUE;
CREATE INDEX pv_agent IF NOT EXISTS FOR (pv:PromptVersion) ON (pv.agent_name);
CREATE INDEX pv_active IF NOT EXISTS FOR (pv:PromptVersion) ON (pv.is_active);

// Relationships
// (EvaluationCriteria)-[:DERIVED_FROM]->(Principle)
// (Evaluation)-[:USES_CRITERIA]->(EvaluationCriteria)
// (FailurePattern)-[:IDENTIFIED_FROM]->(Evaluation)  -- Phase 5
// (PromptVersion)-[:ADDRESSES]->(FailurePattern)     -- Phase 5
```

### 3.2 Node Properties

```yaml
EvaluationCriteria:
  id: string              # "ec:reasoning-cot-completeness"
  name: string            # "Chain-of-Thought Completeness"
  description: string     # "Ï∂îÎ°† Îã®Í≥ÑÍ∞Ä Î™ÖÏãúÏ†ÅÏúºÎ°ú ÎÇòÏó¥ÎêòÏñ¥Ïïº Ìï®"
  principle_id: string    # "p:reasoning" (FK)
  agent_target: string    # "synthesizer" | "intent_classifier" | "*"
  scoring_rubric: string  # JSON or YAML format rubric
  weight: float           # 0.0-1.0, importance in composite score
  version: string         # "1.0.0"
  is_active: boolean      # true
  created_at: datetime

Evaluation:
  id: string              # "eval:2026-02-04-001"
  agent_name: string      # "synthesizer"
  query: string           # Original user query
  response: string        # Agent's response (truncated)
  scores: string          # JSON: {"ec:reasoning-cot": 0.8, "ec:source-citation": 0.9}
  composite_score: float  # Weighted average
  feedback: string        # LLM-generated feedback
  created_at: datetime
  conversation_id: string # Optional session tracking

# Phase 5 nodes (schema only, not implemented yet)
FailurePattern:
  id: string              # "fp:missing-source-citation"
  pattern_type: string    # "output_quality" | "reasoning" | "retrieval"
  description: string
  frequency: int
  affected_agents: list[string]
  root_cause_hypotheses: list[string]
  suggested_fixes: list[string]
  created_at: datetime

PromptVersion:
  id: string              # "pv:synthesizer@1.2.0"
  agent_name: string
  version: string
  content_hash: string    # SHA256 of prompt content
  prompt_path: string     # Path to prompt file or inline
  is_active: boolean
  user_approved: boolean
  parent_version: string  # "pv:synthesizer@1.1.0"
  performance_delta: float # +0.05 (improvement from parent)
  created_at: datetime
```

---

## 4. EvaluationCriteria Ï¥àÍ∏∞ ÏÑ∏Ìä∏

11 PrinciplesÏóêÏÑú ÎèÑÏ∂úÌïú ÌèâÍ∞Ä Í∏∞Ï§Ä (ÏóêÏù¥Ï†ÑÌä∏Î≥Ñ):

### 4.1 Synthesizer ÌèâÍ∞Ä Í∏∞Ï§Ä

| ID | Name | Principle | Description | Weight |
|----|------|-----------|-------------|--------|
| ec:answer-relevance | Answer Relevance | p:reasoning | ÏßàÎ¨∏Ïóê ÏßÅÏ†ëÏ†ÅÏúºÎ°ú ÎãµÎ≥ÄÌïòÎäîÍ∞Ä | 0.20 |
| ec:source-citation | Source Citation | p:grounding | KG Ï∂úÏ≤òÎ•º Î™ÖÏãúÌïòÎäîÍ∞Ä | 0.15 |
| ec:factual-accuracy | Factual Accuracy | p:grounding | KG Îç∞Ïù¥ÌÑ∞ÏôÄ ÏùºÏπòÌïòÎäîÍ∞Ä | 0.20 |
| ec:reasoning-steps | Reasoning Steps | p:reasoning | ÎÖºÎ¶¨Ï†Å Ï∂îÎ°† Í≥ºÏ†ïÏù¥ Î≥¥Ïù¥ÎäîÍ∞Ä | 0.15 |
| ec:completeness | Completeness | p:memory | Í¥ÄÎ†® Ï†ïÎ≥¥Î•º ÎàÑÎùΩÌïòÏßÄ ÏïäÏïòÎäîÍ∞Ä | 0.15 |
| ec:conciseness | Conciseness | p:planning | Î∂àÌïÑÏöîÌïú Ï†ïÎ≥¥ ÏóÜÏù¥ Í∞ÑÍ≤∞ÌïúÍ∞Ä | 0.10 |
| ec:safety | Safety | p:guardrails | Ïú†Ìï¥/Î∂ÄÏ†ÅÏ†à ÎÇ¥Ïö©Ïù¥ ÏóÜÎäîÍ∞Ä | 0.05 |

### 4.2 Intent Classifier ÌèâÍ∞Ä Í∏∞Ï§Ä

| ID | Name | Principle | Description | Weight |
|----|------|-----------|-------------|--------|
| ec:intent-accuracy | Intent Accuracy | p:perception | ÏùòÎèÑÎ•º Ï†ïÌôïÌûà Î∂ÑÎ•òÌñàÎäîÍ∞Ä | 0.40 |
| ec:entity-extraction | Entity Extraction | p:perception | ÏóîÌã∞Ìã∞Î•º Ï†ïÌôïÌûà Ï∂îÏ∂úÌñàÎäîÍ∞Ä | 0.40 |
| ec:scope-detection | Scope Detection | p:guardrails | out_of_scopeÎ•º Ï†ÅÏ†àÌûà Í∞êÏßÄÌïòÎäîÍ∞Ä | 0.20 |

### 4.3 Search Planner ÌèâÍ∞Ä Í∏∞Ï§Ä

| ID | Name | Principle | Description | Weight |
|----|------|-----------|-------------|--------|
| ec:template-selection | Template Selection | p:planning | Ï†ÅÏ†àÌïú Cypher ÌÖúÌîåÎ¶øÏùÑ ÏÑ†ÌÉùÌñàÎäîÍ∞Ä | 0.50 |
| ec:retrieval-mode | Retrieval Mode | p:tool-use | graph/vector/hybrid ÏÑ†ÌÉùÏù¥ Ï†ÅÏ†àÌïúÍ∞Ä | 0.30 |
| ec:parameter-binding | Parameter Binding | p:reasoning | ÌååÎùºÎØ∏ÌÑ∞Î•º Ï†ïÌôïÌûà Î∞îÏù∏Îî©ÌñàÎäîÍ∞Ä | 0.20 |

---

## 5. Critic Agent Íµ¨ÌòÑ

### 5.1 ÌååÏùº Íµ¨Ï°∞

```
src/
‚îú‚îÄ‚îÄ critic/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py       # CriticEvaluator class
‚îÇ   ‚îú‚îÄ‚îÄ criteria.py        # Load/manage EvaluationCriteria
‚îÇ   ‚îú‚îÄ‚îÄ scorer.py          # Multi-criteria scoring logic
‚îÇ   ‚îî‚îÄ‚îÄ feedback.py        # LLM-based feedback generation
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ nodes/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ critic.py      # Critic node for pipeline (optional)
‚îÇ   ‚îî‚îÄ‚îÄ graph.py           # Add critic node after synthesizer
config/
‚îî‚îÄ‚îÄ evaluation_criteria.yaml  # Criteria definitions
```

### 5.2 CriticEvaluator Class

```python
class CriticEvaluator:
    """Evaluates agent outputs against EvaluationCriteria."""

    def __init__(self):
        self.criteria = load_criteria_from_yaml()
        self.llm = get_provider()

    def evaluate(
        self,
        agent_name: str,
        query: str,
        response: str,
        context: dict,  # kg_results, vector_results, etc.
    ) -> Evaluation:
        """Score response against all criteria for this agent."""
        relevant_criteria = self.get_criteria_for_agent(agent_name)
        scores = {}

        for criterion in relevant_criteria:
            score = self.score_criterion(criterion, query, response, context)
            scores[criterion.id] = score

        composite = self.calculate_composite(scores, relevant_criteria)
        feedback = self.generate_feedback(scores, relevant_criteria)

        return Evaluation(
            id=generate_eval_id(),
            agent_name=agent_name,
            query=query,
            response=response[:500],
            scores=json.dumps(scores),
            composite_score=composite,
            feedback=feedback,
            created_at=datetime.now(),
        )

    def score_criterion(
        self,
        criterion: EvaluationCriteria,
        query: str,
        response: str,
        context: dict,
    ) -> float:
        """Score a single criterion using LLM or heuristics."""
        # Use LLM to score based on rubric
        prompt = f"""
        Evaluate this response on the criterion: {criterion.name}

        Criterion: {criterion.description}
        Rubric: {criterion.scoring_rubric}

        Query: {query}
        Response: {response}

        Score from 0.0 to 1.0:
        """
        # ... LLM call and parse score
```

### 5.3 Pipeline Integration

Option A: **Post-pipeline hook** (non-blocking)
```python
def run_agent(query: str) -> dict:
    result = graph.invoke(initial_state)

    # Async evaluation (doesn't block response)
    asyncio.create_task(evaluate_and_store(result))

    return result
```

Option B: **Pipeline node** (blocking, adds latency)
```python
# In graph.py
graph.add_node("critic", critic_node)
graph.add_edge("synthesize_answer", "critic")
```

**Recommendation**: Start with Option A (post-pipeline hook) to avoid latency impact.

---

## 6. P2: Document Pipeline

### 6.1 ÌååÏùº Íµ¨Ï°∞

```
src/
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ crawler.py         # URL/PDF text extraction
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py         # Text chunking strategies
‚îÇ   ‚îî‚îÄ‚îÄ linker.py          # Document ‚Üí KG relationship extraction
scripts/
‚îú‚îÄ‚îÄ ingest_document.py     # CLI for single document
‚îî‚îÄ‚îÄ ingest_batch.py        # Batch ingestion
src/ui/
‚îî‚îÄ‚îÄ app.py                 # Add upload widget
```

### 6.2 Crawler (`src/ingestion/crawler.py`)

```python
class DocumentCrawler:
    """Extract text from URL or PDF."""

    def crawl_url(self, url: str) -> Document:
        """Fetch and extract text from URL."""
        # Use httpx + BeautifulSoup or Tavily extract
        pass

    def crawl_pdf(self, file_path: Path) -> Document:
        """Extract text from PDF using PyMuPDF."""
        import fitz  # pymupdf
        doc = fitz.open(file_path)
        text = "\n".join(page.get_text() for page in doc)
        return Document(
            title=file_path.stem,
            content=text,
            source_path=str(file_path),
        )
```

### 6.3 Document ‚Üí KG Linker (`src/ingestion/linker.py`)

```python
class DocumentLinker:
    """Extract relationships between Document and KG entities."""

    def link_document(self, doc: Document) -> list[Relationship]:
        """Use LLM to identify Methods/Implementations mentioned."""

        # Load entity catalog for context
        catalog = load_entity_catalog()

        prompt = f"""
        Given this document, identify which Agentic AI concepts it discusses.

        Document: {doc.content[:3000]}

        Known entities in our Knowledge Graph:
        - Methods: {catalog['methods'][:20]}
        - Implementations: {catalog['implementations']}

        For each mentioned entity, specify the relationship:
        - PROPOSES: Document introduces/proposes this method
        - EVALUATES: Document evaluates/benchmarks this method
        - DESCRIBES: Document describes this implementation
        - USES: Document uses this implementation

        Output JSON:
        [
          {{"entity_id": "m:react", "relationship": "EVALUATES", "evidence": "quote..."}},
          ...
        ]
        """
        # ... LLM call and parse
```

### 6.4 Streamlit Upload UI

```python
# In src/ui/app.py sidebar
with st.expander("üìÑ Add Document"):
    uploaded_file = st.file_uploader("Upload PDF", type=["pdf"])
    url_input = st.text_input("Or enter URL")

    if st.button("Process"):
        if uploaded_file:
            doc = crawler.crawl_pdf(uploaded_file)
        elif url_input:
            doc = crawler.crawl_url(url_input)

        # Extract and show proposed links
        links = linker.link_document(doc)
        st.json(links)

        if st.button("Approve & Add to KG"):
            # Save document node and relationships
            pass
```

---

## 7. Íµ¨ÌòÑ ÏàúÏÑú

### Step 1: Schema & Seed (Day 1)
- [ ] `neo4j/schema.cypher` ‚Äî Add EvaluationCriteria, Evaluation constraints
- [ ] `config/evaluation_criteria.yaml` ‚Äî Define initial 15 criteria
- [ ] `neo4j/seed_evaluation.cypher` ‚Äî Seed EvaluationCriteria nodes

### Step 2: Critic Core (Day 2)
- [ ] `src/critic/criteria.py` ‚Äî Load criteria from YAML/Neo4j
- [ ] `src/critic/scorer.py` ‚Äî LLM-based scoring per criterion
- [ ] `src/critic/evaluator.py` ‚Äî CriticEvaluator orchestration
- [ ] `src/critic/feedback.py` ‚Äî Generate improvement feedback

### Step 3: Pipeline Integration (Day 3)
- [ ] `src/agents/graph.py` ‚Äî Add post-pipeline evaluation hook
- [ ] `src/api/routes.py` ‚Äî Add `/evaluations` endpoint
- [ ] `src/ui/app.py` ‚Äî Show evaluation scores in response

### Step 4: Document Crawler (Day 4)
- [ ] `src/ingestion/crawler.py` ‚Äî URL + PDF extraction
- [ ] `src/ingestion/chunker.py` ‚Äî Text chunking
- [ ] `scripts/ingest_document.py` ‚Äî CLI tool

### Step 5: Document Linker (Day 5)
- [ ] `src/ingestion/linker.py` ‚Äî LLM-based entity linking
- [ ] `src/ui/app.py` ‚Äî Upload widget + approval UI
- [ ] Integration test with sample PDF

### Step 6: Testing & Polish (Day 6)
- [ ] End-to-end test: query ‚Üí response ‚Üí evaluation ‚Üí storage
- [ ] End-to-end test: PDF ‚Üí chunks ‚Üí embedding ‚Üí KG links
- [ ] Update CHANGELOG.md, CLAUDE.md

---

## 8. Í≤ÄÏ¶ù Î∞©Î≤ï

### Critic Agent Í≤ÄÏ¶ù
```bash
# 1. Generate evaluation criteria
poetry run python scripts/seed_evaluation_criteria.py

# 2. Run agent and check evaluation
poetry run python scripts/test_agent.py --query "What is ReAct?" --evaluate

# 3. Query evaluations
# GET /evaluations?agent=synthesizer&limit=10
```

### Document Pipeline Í≤ÄÏ¶ù
```bash
# 1. Ingest a PDF
poetry run python scripts/ingest_document.py --pdf papers/react.pdf

# 2. Check proposed links
# Streamlit UI shows: "Document mentions: m:react (PROPOSES), impl:langchain (USES)"

# 3. Approve and verify in Neo4j
MATCH (d:Document)-[r]->(m:Method) RETURN d.title, type(r), m.name
```

---

## 9. ÏùòÏ°¥ÏÑ±

```toml
# pyproject.toml additions
pymupdf = "^1.24.0"        # Already added
beautifulsoup4 = "^4.12.0" # For URL crawling
```

---

## 10. Î¶¨Ïä§ÌÅ¨ & ÏôÑÌôî

| Risk | Mitigation |
|------|------------|
| LLM scoring variance | Use rubrics with examples; consider few-shot prompts |
| Evaluation latency | Run async, don't block response |
| Document linking accuracy | Show proposals for human approval before committing |
| Storage growth (Evaluation nodes) | Add retention policy; archive old evaluations |
