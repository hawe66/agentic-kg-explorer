CHANGELOG.mdë¥¼ ë³´ë‹ˆ í”„ë¡œì íŠ¸ê°€ ìƒë‹¹íˆ ë§ì´ ì§„í–‰ë˜ì—ˆë„¤ìš”! í”¼ë“œë°± í¬ì¸íŠ¸ë“¤ì„ ì •ë¦¬í•˜ê³  í•´ê²° ë°©í–¥ì„ ì œì•ˆí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

---

## ë¬¸ì œ ì •ë¦¬ ë° í•´ê²° ë°©í–¥

### 1. Critic: ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ ë¶€ì¬

**ë¬¸ì œ:** Criticì´ ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ë‹µí•˜ê¸°ì— ì¶©ë¶„í•œì§€ íŒë‹¨í•˜ì§€ ëª»í•¨.

**í•´ê²° ë°©í–¥:**
```python
# src/agents/nodes/critic.py (ì‹ ê·œ)
def evaluate_sufficiency(query: str, intent: str, results: dict) -> dict:
    """ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ì¶©ë¶„í•œì§€ í‰ê°€"""
    
    checks = {
        "has_results": bool(results.get("kg_results") or results.get("vector_results")),
        "entity_coverage": check_entity_coverage(query, results),  # ì§ˆë¬¸ì˜ ì—”í‹°í‹°ê°€ ê²°ê³¼ì— ìˆëŠ”ê°€
        "intent_fulfillment": check_intent_fulfillment(intent, results),  # comparisonì¸ë° 1ê°œë§Œ?
        "semantic_relevance": check_semantic_relevance(query, results),  # vector score ê¸°ë°˜
    }
    
    is_sufficient = all(checks.values())
    
    return {
        "is_sufficient": is_sufficient,
        "checks": checks,
        "recommendation": "proceed" if is_sufficient else "expand_search" or "out_of_scope"
    }
```

íŒŒì´í”„ë¼ì¸ì— Critic ë…¸ë“œ ì¶”ê°€:
```
retrieve_from_graph â†’ critic_evaluate â†’ [sufficient?] â†’ synthesize / web_search
```

---

### 2. Intent ë¶„ë¥˜ ì²´ê³„ ë¹ˆì•½

**ë¬¸ì œ:** lookup, path, comparison, expansion 4ê°œë¡œëŠ” ë‹¤ì–‘í•œ ì§ˆë¬¸ ì»¤ë²„ ë¶ˆê°€.

**í•´ê²° ë°©í–¥:**

```python
# config/intents.yaml (ì‹ ê·œ)
intents:
  # ì½ê¸°
  lookup:
    description: "íŠ¹ì • ì—”í‹°í‹° ì¡°íšŒ"
    examples: ["ReActê°€ ë­ì•¼?", "m:reactì˜ maturityëŠ”?"]
  exploration:
    description: "ê´€ê³„ íƒìƒ‰ (1-hop)"
    examples: ["Memory ê´€ë ¨ MethodëŠ”?"]
  path_trace:
    description: "ê²½ë¡œ ì¶”ì  (multi-hop)"
    examples: ["CoTì—ì„œ LangChainê¹Œì§€ ì–´ë–»ê²Œ ì—°ê²°ë¼?"]
  aggregation:
    description: "ì§‘ê³„/í†µê³„"
    examples: ["Principleë³„ Method ìˆ˜", "production Method ëª‡ ê°œ?"]
  
  # ë¶„ì„
  comparison:
    description: "ì—”í‹°í‹° ë¹„êµ"
    examples: ["LangChain vs CrewAI", "Planningê³¼ Reasoning ì°¨ì´"]
  recommendation:
    description: "ì¡°ê±´ ê¸°ë°˜ ì¶”ì²œ"
    examples: ["Reasoning ê°œì„ í•  Method ì¶”ì²œí•´ì¤˜"]
  coverage_check:
    description: "KG í’ˆì§ˆ/ê°­ ë¶„ì„"
    examples: ["Paper ì—†ëŠ” Method ëª©ë¡"]
  
  # ë©”íƒ€
  definition:
    description: "ìŠ¤í‚¤ë§ˆ/êµ¬ì¡° ì„¤ëª…"
    examples: ["ADDRESSES ê´€ê³„ê°€ ë­ì•¼?"]
  
  # ì“°ê¸°
  update:
    description: "ë°ì´í„° ì¶”ê°€/ìˆ˜ì • ì œì•ˆ"
    examples: ["m:self-rag ì¶”ê°€í•´ì¤˜"]
  
  # ì™¸ë¶€
  expansion:
    description: "ë„ë©”ì¸ ë‚´ì§€ë§Œ ì›¹ ê²€ìƒ‰ í•„ìš”"
    examples: ["ìµœì‹  MCP ë³€ê²½ì‚¬í•­"]
  out_of_scope:
    description: "ë„ë©”ì¸ ë¬´ê´€"
    examples: ["ì˜¤ëŠ˜ ë‚ ì”¨", "Why do we live?"]
```

---

### 3. Search Planner í•˜ë“œì½”ë”©

**ë¬¸ì œ:** Cypher í…œí”Œë¦¿ê³¼ ê²½ë¡œ ë°©í–¥ì´ ì½”ë“œì— ê³ ì •ë¨.

**í•´ê²° ë°©í–¥:**

```yaml
# config/cypher_templates.yaml (ì‹ ê·œ)
templates:
  lookup_principle:
    intent: lookup
    entity_types: [Principle]
    cypher: |
      MATCH (p:Principle)
      WHERE toLower(p.name) CONTAINS toLower($entity) OR p.id = $entity
      OPTIONAL MATCH (m:Method)-[a:ADDRESSES]->(p)
      RETURN p, collect({method: m, role: a.role}) as methods
      
  lookup_method:
    intent: lookup
    entity_types: [Method]
    cypher: |
      MATCH (m:Method)
      WHERE toLower(m.name) CONTAINS toLower($entity) OR m.id = $entity
      OPTIONAL MATCH (m)-[a:ADDRESSES]->(p:Principle)
      OPTIONAL MATCH (i:Implementation)-[impl:IMPLEMENTS]->(m)
      RETURN m, collect(DISTINCT p) as principles, collect(DISTINCT i) as implementations
      
  comparison_principle:
    intent: comparison
    entity_types: [Principle, Principle]
    cypher: |
      MATCH (p1:Principle), (p2:Principle)
      WHERE (toLower(p1.name) CONTAINS toLower($entity1) OR p1.id = $entity1)
        AND (toLower(p2.name) CONTAINS toLower($entity2) OR p2.id = $entity2)
      OPTIONAL MATCH (m1:Method)-[:ADDRESSES]->(p1)
      OPTIONAL MATCH (m2:Method)-[:ADDRESSES]->(p2)
      OPTIONAL MATCH (m1)-[:ADDRESSES]->(p2)  // êµì§‘í•©
      RETURN p1, p2, collect(DISTINCT m1) as methods1, collect(DISTINCT m2) as methods2

  aggregation_by_principle:
    intent: aggregation
    cypher: |
      MATCH (p:Principle)<-[:ADDRESSES]-(m:Method)
      RETURN p.name, count(m) as method_count
      ORDER BY method_count DESC

  multi_hop_path:
    intent: path_trace
    cypher: |
      MATCH path = (start)-[*1..3]-(end)
      WHERE start.id = $start_id AND end.id = $end_id
      RETURN path
```

Search Plannerê°€ YAMLì—ì„œ í…œí”Œë¦¿ ë¡œë“œ:
```python
def select_template(intent: str, entity_types: list[str]) -> dict:
    templates = load_yaml("config/cypher_templates.yaml")
    for t in templates:
        if t["intent"] == intent and matches_entity_types(t, entity_types):
            return t
    return None
```

---

### 4. Intent í”„ë¡¬í”„íŠ¸ì— KG ì—”í‹°í‹° ì •ë³´ ë¶€ì¬

**ë¬¸ì œ:** LLMì´ ì‹¤ì œ KGì— ì–´ë–¤ ì—”í‹°í‹°ê°€ ìˆëŠ”ì§€ ëª¨ë¦„.

**í•´ê²° ë°©í–¥:**

```python
# scripts/generate_entity_catalog.py (ì‹ ê·œ)
def generate_entity_catalog():
    """Neo4jì—ì„œ ì—”í‹°í‹° ëª©ë¡ ì¶”ì¶œí•˜ì—¬ ìºì‹œ"""
    
    with Neo4jClient() as client:
        principles = client.query("MATCH (p:Principle) RETURN p.name, p.id")
        methods = client.query("MATCH (m:Method) RETURN m.name, m.id, m.aliases")
        implementations = client.query("MATCH (i:Implementation) RETURN i.name, i.id, i.aliases")
        standards = client.query("MATCH (s:Standard) RETURN s.name, s.id")
    
    catalog = {
        "principles": [{"name": p["name"], "id": p["id"]} for p in principles],
        "methods": [...],
        "implementations": [...],
        "standards": [...],
        "aliases": build_alias_map(methods, implementations),  # CoT â†’ m:cot
        "generated_at": datetime.now().isoformat()
    }
    
    with open("data/entity_catalog.json", "w") as f:
        json.dump(catalog, f)
    
    return catalog
```

Intent Classifier í”„ë¡¬í”„íŠ¸ì— ì£¼ì…:
```python
ENTITY_CATALOG = load_json("data/entity_catalog.json")

INTENT_PROMPT = f"""
You are classifying queries for an Agentic AI knowledge graph.

Known entities:
- Principles: {', '.join(p['name'] for p in ENTITY_CATALOG['principles'])}
- Methods (examples): {', '.join(m['name'] for m in ENTITY_CATALOG['methods'][:15])}...
- Implementations: {', '.join(i['name'] for i in ENTITY_CATALOG['implementations'])}

Aliases: CoT=Chain-of-Thought, RAG=Retrieval-Augmented Generation, ...

Extract entities using EXACT names from the list above.
...
"""
```

---

### 5. Cypher ì¿¼ë¦¬ ì œí•œì  (í”„ë¡œì íŠ¸ ëª©í‘œ: ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•)

**ë¬¸ì œ:** ì±—ë´‡ì´ ì•„ë‹ˆë¼ ì¢‹ì€ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ë§Œë“œëŠ” ê²Œ ëª©í‘œì¸ë°, í˜„ì¬ Cypherê°€ ë„ˆë¬´ ë‹¨ìˆœí•¨.

**í•´ê²° ë°©í–¥:**

ì§€ì‹ ê·¸ë˜í”„ ê´€ë¦¬/ë¶„ì„ìš© Cypher ì¶”ê°€:
```yaml
# config/cypher_templates.yamlì— KG ê´€ë¦¬ìš© í…œí”Œë¦¿ ì¶”ê°€

# í’ˆì§ˆ ë¶„ì„
coverage_methods_without_paper:
  intent: coverage_check
  description: "ë…¼ë¬¸ ì—°ê²° ì—†ëŠ” Method"
  cypher: |
    MATCH (m:Method)
    WHERE NOT (m)<-[:PROPOSES]-(:Document) AND m.seminal_source IS NULL
    RETURN m.id, m.name, m.year_introduced
    ORDER BY m.year_introduced DESC

coverage_orphan_implementations:
  intent: coverage_check
  description: "Method ì—°ê²° ì—†ëŠ” Implementation"
  cypher: |
    MATCH (i:Implementation)
    WHERE NOT (i)-[:IMPLEMENTS]->(:Method)
    RETURN i.id, i.name

coverage_principle_distribution:
  intent: aggregation
  description: "Principleë³„ Method/Implementation ë¶„í¬"
  cypher: |
    MATCH (p:Principle)
    OPTIONAL MATCH (m:Method)-[:ADDRESSES]->(p)
    OPTIONAL MATCH (i:Implementation)-[:IMPLEMENTS]->(m)
    RETURN p.name, 
           count(DISTINCT m) as method_count,
           count(DISTINCT i) as impl_count
    ORDER BY method_count DESC

# ê²½ë¡œ ë¶„ì„
full_path_principle_to_standard:
  intent: path_trace
  description: "Principle â†’ Method â†’ Implementation â†’ Standard ì „ì²´ ê²½ë¡œ"
  cypher: |
    MATCH path = (p:Principle)<-[:ADDRESSES]-(m:Method)
                 <-[:IMPLEMENTS]-(i:Implementation)
                 -[:COMPLIES_WITH]->(sv:StandardVersion)
    WHERE p.id = $principle_id
    RETURN path
```

---

### 6. Confidence ê³„ì‚° ê·¼ë³¸ ì¬ì„¤ê³„

**ë¬¸ì œ:** ê²°ê³¼ ìˆ˜ ê¸°ë°˜ confidenceëŠ” ì™„ì „íˆ ì˜ëª»ë¨.

**í•´ê²° ë°©í–¥:**

```python
# src/agents/nodes/synthesizer.py

def calculate_confidence(
    query: str,
    intent: str,
    entities: list[str],
    kg_results: list,
    vector_results: list
) -> float:
    """ë‹¤ì°¨ì› confidence ê³„ì‚°"""
    
    if not kg_results and not vector_results:
        return 0.0
    
    scores = []
    
    # 1. ì—”í‹°í‹° ë§¤ì¹­ (0.0 - 1.0)
    # ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ ì—”í‹°í‹°ê°€ ê²°ê³¼ì— ìˆëŠ”ê°€?
    matched = sum(1 for e in entities if entity_in_results(e, kg_results))
    entity_score = matched / len(entities) if entities else 0.5
    scores.append(("entity_match", entity_score, 0.3))
    
    # 2. Intent ì¶©ì¡±ë„ (0.0 - 1.0)
    # comparisonì¸ë° ê²°ê³¼ê°€ 1ê°œ? lookupì¸ë° ì •í™• ë§¤ì¹­?
    intent_score = calculate_intent_fulfillment(intent, kg_results, vector_results)
    scores.append(("intent_fulfillment", intent_score, 0.3))
    
    # 3. ë°ì´í„° ì™„ì„±ë„ (0.0 - 1.0)
    # description, seminal_source ë“± í•µì‹¬ í•„ë“œ ì±„ì›Œì§ ë¹„ìœ¨
    completeness = calculate_completeness(kg_results)
    scores.append(("completeness", completeness, 0.2))
    
    # 4. Vector ìœ ì‚¬ë„ (0.0 - 1.0)
    if vector_results:
        avg_vector_score = sum(r["score"] for r in vector_results) / len(vector_results)
        scores.append(("vector_similarity", avg_vector_score, 0.2))
    else:
        scores.append(("vector_similarity", 0.5, 0.2))  # neutral
    
    # ê°€ì¤‘ í‰ê· 
    total = sum(score * weight for _, score, weight in scores)
    
    return round(total, 2)
```

---

### 7. LangChain Docs í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸

**ë¬¸ì œ:** LangChain docsë¥¼ data/papersì— ë„£ê³  ì‹¶ìŒ. ë²”ìš© í¬ë¡¤ëŸ¬ í•„ìš”.

**í•´ê²° ë°©í–¥:**

```python
# scripts/crawl_docs.py (ì‹ ê·œ)

"""
ë²”ìš© ë¬¸ì„œ í¬ë¡¤ëŸ¬.

Usage:
  poetry run python scripts/crawl_docs.py --url "https://python.langchain.com/docs" --output data/papers/langchain
  poetry run python scripts/crawl_docs.py --sitemap "https://example.com/sitemap.xml" --output data/papers/example
  poetry run python scripts/crawl_docs.py --urls-file urls.txt --output data/papers/custom
"""

import argparse
import hashlib
from pathlib import Path
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup
from markdownify import markdownify

def crawl_page(url: str, client: httpx.Client) -> dict:
    """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
    resp = client.get(url)
    soup = BeautifulSoup(resp.text, "html.parser")
    
    # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ (ì‚¬ì´íŠ¸ë³„ selector ì¡°ì • ê°€ëŠ¥)
    content = soup.select_one("main, article, .content, .markdown-body")
    if not content:
        content = soup.body
    
    # Markdown ë³€í™˜
    md = markdownify(str(content), heading_style="ATX")
    
    return {
        "url": url,
        "title": soup.title.string if soup.title else url,
        "content_md": md,
        "crawled_at": datetime.now().isoformat()
    }

def crawl_site(base_url: str, output_dir: Path, max_pages: int = 100):
    """ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ë§í¬ ë”°ë¼ê°€ê¸°)"""
    visited = set()
    queue = [base_url]
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with httpx.Client(follow_redirects=True, timeout=30) as client:
        while queue and len(visited) < max_pages:
            url = queue.pop(0)
            if url in visited:
                continue
            
            try:
                page = crawl_page(url, client)
                visited.add(url)
                
                # íŒŒì¼ ì €ì¥
                filename = hashlib.md5(url.encode()).hexdigest()[:12] + ".md"
                filepath = output_dir / filename
                
                with open(filepath, "w") as f:
                    f.write(f"---\n")
                    f.write(f"source_url: {url}\n")
                    f.write(f"title: {page['title']}\n")
                    f.write(f"crawled_at: {page['crawled_at']}\n")
                    f.write(f"---\n\n")
                    f.write(page["content_md"])
                
                # ë§í¬ ì¶”ì¶œ (ê°™ì€ ë„ë©”ì¸ë§Œ)
                soup = BeautifulSoup(page["content_md"], "html.parser")
                for a in soup.find_all("a", href=True):
                    link = urljoin(url, a["href"])
                    if urlparse(link).netloc == urlparse(base_url).netloc:
                        if link not in visited:
                            queue.append(link)
                
                print(f"[{len(visited)}/{max_pages}] {url}")
                
            except Exception as e:
                print(f"Error crawling {url}: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--url", help="Base URL to crawl")
    parser.add_argument("--sitemap", help="Sitemap XML URL")
    parser.add_argument("--urls-file", help="File with URLs (one per line)")
    parser.add_argument("--output", required=True, help="Output directory")
    parser.add_argument("--max-pages", type=int, default=100)
    args = parser.parse_args()
    
    if args.url:
        crawl_site(args.url, Path(args.output), args.max_pages)
    elif args.sitemap:
        crawl_from_sitemap(args.sitemap, Path(args.output))
    elif args.urls_file:
        crawl_from_file(args.urls_file, Path(args.output))
```

---

### 8. Streamlit UI ê°œì„ 

**ë¬¸ì œë“¤:**
- Example query í´ë¦­ ì‹œ agentê°€ ì‘ë‹µ ì•ˆ í•¨
- Web result í°íŠ¸ ë„ˆë¬´ í¼
- KG ì¶”ê°€ UIê°€ ìŠ¤í¬ë¡¤ ë§¨ ìœ„ì— ìˆì–´ì„œ ë¶ˆí¸
- Local docs ì—…ë¡œë“œ UI ì—†ìŒ

**í•´ê²° ë°©í–¥:**

```python
# streamlit_app.py ê°œì„  ì‚¬í•­

# 1. Example query í´ë¦­ â†’ ìë™ ì‹¤í–‰
if st.button("What is ReAct?", key="example_react"):
    st.session_state.query = "What is ReAct?"
    st.session_state.auto_submit = True

if st.session_state.get("auto_submit"):
    run_query(st.session_state.query)
    st.session_state.auto_submit = False

# 2. Web result í°íŠ¸ í¬ê¸° ì¡°ì •
st.markdown("""
<style>
.web-result { font-size: 0.9rem; }
.web-result-title { font-size: 1rem; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

# 3. KG ì¶”ê°€ UIë¥¼ Floating Panelë¡œ
with st.sidebar:
    st.header("Add to Knowledge Graph")
    # ë˜ëŠ” modal dialog ì‚¬ìš©

# ë˜ëŠ” expanderë¥¼ chat í•˜ë‹¨ì— ê³ ì •
with st.expander("ğŸ“¥ Add to Knowledge Graph", expanded=False):
    selected_results = st.multiselect("Select results to add", options=...)
    if st.button("Add Selected"):
        add_to_kg(selected_results)

# 4. Local docs ì—…ë¡œë“œ UI
st.sidebar.header("ğŸ“„ Upload Documents")
uploaded_files = st.sidebar.file_uploader(
    "Upload papers/docs (PDF, MD, TXT)",
    accept_multiple_files=True,
    type=["pdf", "md", "txt"]
)

if uploaded_files:
    for file in uploaded_files:
        save_path = Path("data/papers") / file.name
        save_path.write_bytes(file.read())
        st.sidebar.success(f"Saved: {file.name}")
    
    if st.sidebar.button("Process & Embed"):
        process_uploaded_docs(uploaded_files)
```

---

### 9. ìƒˆ Documentë¥¼ KGì— ìë™ ì—°ê²°

**ë¬¸ì œ:** ìƒˆë¡œ ì¶”ê°€ëœ ë¬¸ì„œê°€ Principle/Method/Implementationê³¼ ìë™ ì—°ê²°ë˜ì§€ ì•ŠìŒ.

**í•´ê²° ë°©í–¥:**

```python
# src/agents/nodes/document_linker.py (ì‹ ê·œ)

def link_document_to_kg(doc_id: str, doc_content: str) -> list[dict]:
    """ë¬¸ì„œ ë‚´ìš© ë¶„ì„í•˜ì—¬ KG ì—”í‹°í‹°ì™€ ì—°ê²°"""
    
    # 1. ë¬¸ì„œ ì„ë² ë”©
    doc_embedding = embed(doc_content)
    
    # 2. ìœ ì‚¬ Method/Implementation ê²€ìƒ‰
    similar_nodes = vector_store.query(doc_embedding, top_k=10)
    
    # 3. LLMìœ¼ë¡œ ê´€ê³„ ì¶”ë¡ 
    prompt = f"""
    Document content (excerpt):
    {doc_content[:2000]}
    
    Similar nodes found:
    {format_nodes(similar_nodes)}
    
    Determine relationships:
    - Does this document PROPOSE a new Method? If so, which one?
    - Does it EXTEND or VARIANT_OF an existing Method?
    - Does it EVALUATE any Methods/Implementations?
    
    Return JSON:
    {{
      "proposes": ["method_name"],
      "extends": {{"new": "method_name", "base": "existing_method_id"}},
      "evaluates": ["method_id1", "impl_id2"]
    }}
    """
    
    relationships = llm.generate(prompt)
    
    # 4. ì‚¬ìš©ì ìŠ¹ì¸ ëŒ€ê¸°ì—´ì— ì¶”ê°€
    pending_links = []
    for rel_type, targets in relationships.items():
        for target in targets:
            pending_links.append({
                "document_id": doc_id,
                "relationship": rel_type,
                "target": target,
                "confidence": calculate_link_confidence(...),
                "status": "pending_approval"
            })
    
    return pending_links
```

ì›Œí¬í”Œë¡œìš°:
```
ë¬¸ì„œ ì—…ë¡œë“œ â†’ ì„ë² ë”© + KG ìœ ì‚¬ë„ ê²€ìƒ‰ â†’ LLM ê´€ê³„ ì¶”ë¡  
    â†’ Pending Queueì— ì €ì¥ â†’ UIì—ì„œ ìŠ¹ì¸/ê±°ì ˆ â†’ Neo4jì— ê´€ê³„ ìƒì„±
```

---

## ìš°ì„ ìˆœìœ„ ì •ë¦¬

### P0 (ì¦‰ì‹œ) âœ… DONE
- [x] Confidence ê³„ì‚° ì¬ì„¤ê³„ â†’ Multi-dimensional (entity/intent/completeness/vector)
- [x] `out_of_scope` intent ì¶”ê°€ â†’ 5 intents now (+ out_of_scope)
- [x] Entity Catalog ìƒì„± + í”„ë¡¬í”„íŠ¸ ì£¼ì… â†’ `scripts/generate_entity_catalog.py`

### P1 (ë‹¨ê¸°) âœ… DONE
- [x] Intent ë¶„ë¥˜ ì²´ê³„ í™•ì¥ (YAML ê¸°ë°˜) â†’ `config/intents.yaml` (11 intents)
- [x] Cypher í…œí”Œë¦¿ ì™¸ë¶€í™” (YAML) â†’ `config/cypher_templates.yaml` (20+ templates)
- [ ] ~~Critic ë…¸ë“œ ì¶”ê°€ (ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨)~~ â†’ Moved to Phase 4
- [x] Streamlit UI ê°œì„  (example ìë™ì‹¤í–‰, í°íŠ¸, floating panel)

### P2 (ì¤‘ê¸°)
- [ ] ë²”ìš© ë¬¸ì„œ í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸
- [ ] Local docs ì—…ë¡œë“œ UI
- [ ] Document â†’ KG ìë™ ì—°ê²° (link_document_to_kg)
- [x] ~~KG ê´€ë¦¬ìš© Cypher (coverage_check, aggregation)~~ â†’ Included in cypher_templates.yaml
- [x] ~~Graph visualization~~ â†’ `streamlit-agraph` in `src/ui/app.py` (toggle, node colors, overview mode)

### Phase 4 (Critic Agent)
- [ ] EvaluationCriteria nodes derived from Principles
- [ ] Evaluation logic with multi-dimensional scoring
- [ ] Guideline versioning
- [ ] Human-in-the-loop approval gates

ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ íŠ¹ì • í•­ëª© ë” ìƒì„¸í•˜ê²Œ ë³´ê³  ì‹¶ìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!